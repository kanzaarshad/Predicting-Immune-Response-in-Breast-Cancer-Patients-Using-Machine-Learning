{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CebYmp1nXH6d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load dataset (placeholder - replace with actual data loading)\n",
        "data = pd.read_csv(\"immune_response_data.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    # Handle missing values\n",
        "    df = df.dropna(thresh=0.7 * len(df), axis=1)  # Drop columns with >30% missing\n",
        "    df.fillna(df.median(), inplace=True)\n",
        "\n",
        "    # Encode categorical features\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = StandardScaler()\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "data = preprocess_data(data)\n",
        "\n",
        "# Feature Selection\n",
        "X = data.drop(columns=['immune_response'])  # Features\n",
        "y = data['immune_response']  # Target\n",
        "\n",
        "feature_importance_model = RandomForestClassifier(n_estimators=100)\n",
        "feature_importance_model.fit(X, y)\n",
        "feature_importances = pd.Series(feature_importance_model.feature_importances_, index=X.columns)\n",
        "selected_features = feature_importances.nlargest(20).index  # Select top 20 features\n",
        "X = X[selected_features]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Model Training & Evaluation\n",
        "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return {\n",
        "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Precision\": precision_score(y_test, y_pred),\n",
        "        \"Recall\": recall_score(y_test, y_pred),\n",
        "        \"F1 Score\": f1_score(y_test, y_pred),\n",
        "        \"AUC-ROC\": roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    }\n",
        "\n",
        "# Train different models\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"Logistic Regression\": LogisticRegression()\n",
        "}\n",
        "\n",
        "results = {name: train_and_evaluate(model, X_train, X_test, y_train, y_test) for name, model in models.items()}\n",
        "print(pd.DataFrame(results))\n",
        "\n",
        "# Neural Network Model\n",
        "def build_nn(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_model = build_nn(X_train.shape[1])\n",
        "nn_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.15, verbose=1)\n",
        "y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int).flatten()\n",
        "nn_results = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred_nn),\n",
        "    \"Precision\": precision_score(y_test, y_pred_nn),\n",
        "    \"Recall\": recall_score(y_test, y_pred_nn),\n",
        "    \"F1 Score\": f1_score(y_test, y_pred_nn),\n",
        "    \"AUC-ROC\": roc_auc_score(y_test, nn_model.predict(X_test))\n",
        "}\n",
        "print(\"Neural Network Results:\", nn_results)\n"
      ]
    }
  ]
}